import numpy as np
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from keras.callbacks import (
    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
)
import os
import joblib
import pandas as pd
import keras

from app.ml_models.src.data_processing.feature_engineer import FeatureEngineer


class ModelTrainer:
    def __init__(self, config):
        self.config = config
        self.scalers = {}
        self.label_encoders = {}

        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self._create_directories()

    def _create_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        os.makedirs('app/ml_models/models/model_checkpoints', exist_ok=True)
        os.makedirs('app/ml_models/models/trained', exist_ok=True)
        os.makedirs('logs', exist_ok=True)

    def prepare_data(self, df: pd.DataFrame, ticker_column: str = None):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —Ñ–∏—á–∞–º–∏ —Ç—Ä–µ–Ω–¥–∞"""

        # Feature engineering
        feature_engineer = FeatureEngineer(self.config)
        df_processed = feature_engineer.prepare_features(df)

        # –†–∞–∑–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ –∏ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –î–û —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        target_columns = self.config.data_config['target_columns']

        # –í—ã–±–æ—Ä —Ñ–∏—á –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ò–°–ö–õ–Æ–ß–ê–ï–ú —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ —Ñ–∏—á!)
        feature_columns = []
        # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω–æ–≤—ã–µ —Ñ–∏—á–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ–∏—á–µ–π, –Ω–µ —Ü–µ–ª–∏!)
        feature_columns.extend(['open', 'high', 'low', 'close', 'volume'])

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        technical_indicators = self.config.features_config['technical_indicators']
        available_technical = [f for f in technical_indicators if f in df_processed.columns]
        feature_columns.extend(available_technical)

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
        time_features = self.config.features_config['time_features']
        available_time = [f for f in time_features if f in df_processed.columns]
        feature_columns.extend(available_time)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏
        additional_features = ['price_range', 'body_size', 'upper_shadow', 'lower_shadow']
        available_additional = [f for f in additional_features if f in df_processed.columns]
        feature_columns.extend(available_additional)

        # üî• –î–û–ë–ê–í–õ–Ø–ï–ú –§–ò–ß–ò –¢–†–ï–ù–î–ê
        trend_features = self._add_trend_features(df_processed)
        feature_columns.extend(trend_features)

        # –£–ë–ò–†–ê–ï–ú —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ —Ñ–∏—á–µ–π, –µ—Å–ª–∏ –æ–Ω–∏ —Ç–∞–º –µ—Å—Ç—å
        feature_columns = [f for f in feature_columns if f not in target_columns]

        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ–∏—á–∏ ({len(feature_columns)}): {feature_columns}")
        print(f"–¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ({len(target_columns)}): {target_columns}")

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Å–µ —Ñ–∏—á–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        missing_features = set(feature_columns) - set(df_processed.columns)
        if missing_features:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∏—á–∏: {missing_features}")
            feature_columns = [f for f in feature_columns if f in df_processed.columns]

        # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–∞—Å—Å–∏–≤—ã –¥–ª—è —Ñ–∏—á –∏ —Ü–µ–ª–µ–π
        X_data = df_processed[feature_columns].values
        y_data = df_processed[target_columns].values

        print(f"–§–æ—Ä–º–∞ X_data: {X_data.shape}, –§–æ—Ä–º–∞ y_data: {y_data.shape}")

        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        lookback = self.config.data_config['lookback']
        X_seq, y_seq = self._create_sequences(X_data, y_data, lookback)

        print(f"–ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: X_seq {X_seq.shape}, y_seq {y_seq.shape}")

        # üî• –°–û–ó–î–ê–ï–ú –ú–ï–¢–ö–ò –¢–†–ï–ù–î–ê –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        y_trend = self._create_trend_labels(df_processed, lookback)

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
        train_size = int(len(X_seq) * self.config.data_config['train_test_split'])
        val_size = int(len(X_seq) * self.config.data_config['validation_split'])

        X_train = X_seq[:train_size]
        y_train = y_seq[:train_size]
        y_train_trend = y_trend[:train_size]

        X_val = X_seq[train_size:train_size + val_size]
        y_val = y_seq[train_size:train_size + val_size]
        y_val_trend = y_trend[train_size:train_size + val_size]

        X_test = X_seq[train_size + val_size:]
        y_test = y_seq[train_size + val_size:]
        y_test_trend = y_trend[train_size + val_size:]

        print(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ features
        self.scalers['feature'] = StandardScaler()
        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])
        X_train_scaled = self.scalers['feature'].fit_transform(X_train_reshaped)
        X_train = X_train_scaled.reshape(X_train.shape)

        X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])
        X_val_scaled = self.scalers['feature'].transform(X_val_reshaped)
        X_val = X_val_scaled.reshape(X_val.shape)

        X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])
        X_test_scaled = self.scalers['feature'].transform(X_test_reshaped)
        X_test = X_test_scaled.reshape(X_test.shape)

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ targets
        self.scalers['target'] = StandardScaler()
        y_train_scaled = self.scalers['target'].fit_transform(y_train)
        y_val_scaled = self.scalers['target'].transform(y_val)
        y_test_scaled = self.scalers['target'].transform(y_test)

        # üî• –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ targets –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –° –¢–†–ï–ù–î–û–ú
        y_train_dict = {
            'open': y_train_scaled[:, 0],
            'high': y_train_scaled[:, 1],
            'low': y_train_scaled[:, 2],
            'close': y_train_scaled[:, 3],
            'trend': y_train_trend  # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ —Ç—Ä–µ–Ω–¥–∞
        }

        y_val_dict = {
            'open': y_val_scaled[:, 0],
            'high': y_val_scaled[:, 1],
            'low': y_val_scaled[:, 2],
            'close': y_val_scaled[:, 3],
            'trend': y_val_trend
        }

        y_test_dict = {
            'open': y_test_scaled[:, 0],
            'high': y_test_scaled[:, 1],
            'low': y_test_scaled[:, 2],
            'close': y_test_scaled[:, 3],
            'trend': y_test_trend
        }

        data_info = {
            'feature_columns': feature_columns,
            'target_columns': target_columns,
            'sequence_length': lookback,
            'n_features': len(feature_columns),
            'original_features_count': len(feature_columns),
            'target_features_count': len(target_columns),
            'trend_classes': y_train_trend.shape[1] if len(y_train_trend.shape) > 1 else 1
        }

        print(f"–ò—Ç–æ–≥–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: n_features = {data_info['n_features']}")
        print(f"–ö–ª–∞—Å—Å—ã —Ç—Ä–µ–Ω–¥–∞: {data_info['trend_classes']}")

        return (X_train, y_train_dict), (X_val, y_val_dict), (X_test, y_test_dict), data_info

    def _add_trend_features(self, df: pd.DataFrame) -> list:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏—á –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞"""
        trend_features = []

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ float –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å —Ç–∏–ø–∞–º–∏
        numeric_columns = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        df[numeric_columns] = df[numeric_columns].fillna(method='ffill')

        # 1. –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
        for window in [5, 10, 20, 50]:
            df[f'ma_{window}'] = df['close'].rolling(window=window, min_periods=1).mean()
            df[f'ema_{window}'] = df['close'].ewm(span=window, min_periods=1).mean()
            trend_features.extend([f'ma_{window}', f'ema_{window}'])

            # –û—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω—ã –∫ —Å–∫–æ–ª—å–∑—è—â–∏–º —Å—Ä–µ–¥–Ω–∏–º (—Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0)
            df[f'price_ma_ratio_{window}'] = df['close'] / df[f'ma_{window}'].replace(0, 1e-10)
            df[f'price_ema_ratio_{window}'] = df['close'] / df[f'ema_{window}'].replace(0, 1e-10)
            trend_features.extend([f'price_ma_ratio_{window}', f'price_ema_ratio_{window}'])

        # 2. –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞ (–ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ)
        for window in [5, 10, 20]:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º diff —Å –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            price_diff = df['close'].diff(window)
            df[f'trend_direction_{window}'] = np.where(
                price_diff > 0, 1, np.where(price_diff < 0, -1, 0)
            )
            trend_features.append(f'trend_direction_{window}')

        # 3. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–¥–∞
        for window in [10, 20]:
            df[f'trend_volatility_{window}'] = df['close'].rolling(window, min_periods=1).std()
            high_max = df['high'].rolling(window, min_periods=1).max()
            low_min = df['low'].rolling(window, min_periods=1).min()
            df[f'trend_range_{window}'] = (high_max - low_min) / df['close'].replace(0, 1e-10)

            # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
            df[f'trend_volatility_{window}'] = df[f'trend_volatility_{window}'].fillna(0)
            df[f'trend_range_{window}'] = df[f'trend_range_{window}'].fillna(0)

            trend_features.extend([f'trend_volatility_{window}', f'trend_range_{window}'])

        # 4. –ú–æ–º–µ–Ω—Ç—É–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        for window in [5, 10, 14]:
            # RSI-like feature
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()

            # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
            rs = gain / loss.replace(0, 1e-10)
            df[f'momentum_rsi_{window}'] = 100 - (100 / (1 + rs))

            # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN
            df[f'momentum_rsi_{window}'] = df[f'momentum_rsi_{window}'].fillna(50)
            trend_features.append(f'momentum_rsi_{window}')

        # 5. MACD –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        exp1 = df['close'].ewm(span=12, min_periods=1).mean()
        exp2 = df['close'].ewm(span=26, min_periods=1).mean()
        df['macd'] = exp1 - exp2
        df['macd_signal'] = df['macd'].ewm(span=9, min_periods=1).mean()
        df['macd_histogram'] = df['macd'] - df['macd_signal']

        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN
        df['macd'] = df['macd'].fillna(0)
        df['macd_signal'] = df['macd_signal'].fillna(0)
        df['macd_histogram'] = df['macd_histogram'].fillna(0)

        trend_features.extend(['macd', 'macd_signal', 'macd_histogram'])

        # 6. –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ
        for window in [20, 50]:
            df[f'support_{window}'] = df['low'].rolling(window, min_periods=1).min()
            df[f'resistance_{window}'] = df['high'].rolling(window, min_periods=1).max()

            # –û—Ç–Ω–æ—à–µ–Ω–∏—è —Ü–µ–Ω—ã –∫ –ø–æ–¥–¥–µ—Ä–∂–∫–µ/—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—é
            df[f'price_to_support_{window}'] = (df['close'] - df[f'support_{window}']) / df['close'].replace(0, 1e-10)
            df[f'price_to_resistance_{window}'] = (df[f'resistance_{window}'] - df['close']) / df['close'].replace(0,
                                                                                                                   1e-10)

            # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN
            df[f'support_{window}'] = df[f'support_{window}'].fillna(df['close'])
            df[f'resistance_{window}'] = df[f'resistance_{window}'].fillna(df['close'])
            df[f'price_to_support_{window}'] = df[f'price_to_support_{window}'].fillna(0)
            df[f'price_to_resistance_{window}'] = df[f'price_to_resistance_{window}'].fillna(0)

            trend_features.extend([
                f'support_{window}', f'resistance_{window}',
                f'price_to_support_{window}', f'price_to_resistance_{window}'
            ])

        # 7. –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ç—Ä–µ–Ω–¥–∞
        volume_ma = df['volume'].rolling(20, min_periods=1).mean()
        df['volume_ma_ratio'] = df['volume'] / volume_ma.replace(0, 1e-10)
        df['volume_ma_ratio'] = df['volume_ma_ratio'].fillna(1)

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Ü–µ–Ω–∞-–æ–±—ä–µ–º (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        df['price_volume_trend'] = np.where(
            (df['close'].diff() > 0) & (df['volume'].diff() > 0), 1,
            np.where((df['close'].diff() < 0) & (df['volume'].diff() > 0), -1, 0)
        )

        trend_features.extend(['volume_ma_ratio', 'price_volume_trend'])

        # 8. –ü—Ä–æ—Å—Ç—ã–µ, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ñ–∏—á–∏ —Ç—Ä–µ–Ω–¥–∞
        # –¶–µ–Ω–æ–≤–æ–π –º–æ–º–µ–Ω—Ç—É–º
        for period in [1, 3, 5]:
            df[f'price_momentum_{period}'] = (df['close'] - df['close'].shift(period)) / df['close'].shift(
                period).replace(0, 1e-10)
            df[f'price_momentum_{period}'] = df[f'price_momentum_{period}'].fillna(0)
            trend_features.append(f'price_momentum_{period}')

        # –£—Å–∫–æ—Ä–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞
        df['trend_acceleration'] = df['close'].diff().diff()
        df['trend_acceleration'] = df['trend_acceleration'].fillna(0)
        trend_features.append('trend_acceleration')

        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ –Ω–∞—á–∞–ª–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
        df[trend_features] = df[trend_features].fillna(method='bfill').fillna(method='ffill')

        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(trend_features)} —Ñ–∏—á —Ç—Ä–µ–Ω–¥–∞")
        print(f"üìä –ü—Ä–∏–º–µ—Ä —Ñ–∏—á —Ç—Ä–µ–Ω–¥–∞: {trend_features[:10]}...")

        return trend_features

    def _create_trend_labels(self, df: pd.DataFrame, lookback: int, lookforward: int = 5) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫ —Ç—Ä–µ–Ω–¥–∞ –¥–ª—è –º–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ close –≤ —á–∏—Å–ª–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        close_prices = pd.to_numeric(df['close'], errors='coerce').values
        close_prices = np.nan_to_num(close_prices, nan=0.0)

        trend_labels = []

        for i in range(lookback, len(close_prices)):
            if i + lookforward >= len(close_prices):
                # –î–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–∫–æ–≤–æ–π —Ç—Ä–µ–Ω–¥
                trend_labels.append([0, 1, 0])
                continue

            current_price = close_prices[i]
            future_price = close_prices[i + lookforward]

            if current_price == 0:  # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
                trend_labels.append([0, 1, 0])
                continue

            # –ü—Ä–æ—Ü–µ–Ω—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è
            change_percent = (future_price - current_price) / current_price * 100

            # üî• –£–ú–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –¢–†–ï–ù–î–ê
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫—É—é –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ä–æ–≥–∞
            history_window = close_prices[max(0, i - lookback):i]
            if len(history_window) > 1:
                volatility = np.std(history_window)
                threshold = volatility / current_price * 100 * 2  # –ü–æ—Ä–æ–≥ –≤ %
            else:
                threshold = 2.0  # –ü–æ—Ä–æ–≥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

            if change_percent > threshold:  # –í–æ—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥
                trend_label = [0, 0, 1]
            elif change_percent < -threshold:  # –ù–∏—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥
                trend_label = [1, 0, 0]
            else:  # –ë–æ–∫–æ–≤–æ–π —Ç—Ä–µ–Ω–¥
                trend_label = [0, 1, 0]

            trend_labels.append(trend_label)

        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –ø–µ—Ä–≤—ã—Ö lookback —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        for _ in range(lookback):
            trend_labels.insert(0, [0, 1, 0])  # –ë–æ–∫–æ–≤–æ–π —Ç—Ä–µ–Ω–¥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

        trend_array = np.array(trend_labels[:len(close_prices)])

        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
        class_distribution = np.sum(trend_array, axis=0)
        print(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ —Ç—Ä–µ–Ω–¥–∞: DOWN={class_distribution[0]}, "
              f"SIDEWAYS={class_distribution[1]}, UP={class_distribution[2]}")

        return trend_array

    def _create_sequences(self, X_data: np.ndarray, y_data: np.ndarray, lookback: int) -> tuple:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        X_seq, y_seq = [], []

        for i in range(lookback, len(X_data)):
            # –í—Ö–æ–¥–Ω—ã–µ —Ñ–∏—á–∏ (—Ç–æ–ª—å–∫–æ —Ñ–∏—á–∏, –±–µ–∑ —Ü–µ–ª–µ–π)
            X_seq.append(X_data[i - lookback:i, :])
            # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (—Ç–µ–∫—É—â–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥)
            y_seq.append(y_data[i, :])

        return np.array(X_seq), np.array(y_seq)

    def train(self, train_data, val_data, model):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞"""

        X_train, y_train = train_data
        X_val, y_val = val_data

        print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ X_train: {X_train.shape}")
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã y_train:")
        for key, value in y_train.items():
            print(f"  {key}: {value.shape}")

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –æ —Ç—Ä–µ–Ω–¥–µ
        if 'trend' not in y_train:
            print("‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –º–µ—Ç–∫–∏ —Ç—Ä–µ–Ω–¥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! –î–æ–±–∞–≤—å—Ç–µ 'trend' –≤ y_train")
            print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏:", list(y_train.keys()))
            return None

        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ callbacks —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ —Ç—Ä–µ–Ω–¥
        callbacks = [
            EarlyStopping(
                monitor='val_trend_accuracy',  # –û—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ - —Ç–æ—á–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–¥–∞
                patience=self.config.model_config['patience'],
                restore_best_weights=True,
                mode='max',  # –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
                verbose=1
            ),
            ModelCheckpoint(
                filepath='app/ml_models/models/model_checkpoints/best_model.keras',
                monitor='val_trend_accuracy',  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ç—Ä–µ–Ω–¥–∞
                save_best_only=True,
                mode='max',
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_trend_loss',  # –°–ª–µ–¥–∏–º –∑–∞ loss —Ç—Ä–µ–Ω–¥–∞
                factor=0.5,
                patience=8,  # –£–º–µ–Ω—å—à–∏–ª patience –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
                min_lr=1e-7,
                mode='min',
                verbose=1
            ),
            # –î–æ–±–∞–≤–ª—è–µ–º callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ —Ç—Ä–µ–Ω–¥–∞
            keras.callbacks.CSVLogger(
                'app/ml_models/models/training_log.csv',
                separator=',',
                append=False
            )
        ]

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ—Å–æ–≤ –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
        class_weights = self.calculate_trend_class_weights(y_train['trend'])

        # –û–±—É—á–µ–Ω–∏–µ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º —Ç—Ä–µ–Ω–¥–∞
        epochs = self.config.model_config['epochs']

        print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞...")
        print(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {self.config.model_config['batch_size']}")
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {epochs}")

        try:
            history = model.fit(
                X_train, y_train,
                batch_size=self.config.model_config['batch_size'],
                epochs=epochs,
                validation_data=(X_val, y_val),
                callbacks=callbacks,
                verbose=1,
                shuffle=False,  # –í–∞–∂–Ω–æ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
                class_weight={'trend': class_weights}  # –í–µ—Å–∞ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤ —Ç—Ä–µ–Ω–¥–∞
            )


            return history

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
            # Fallback: –æ–±—É—á–µ–Ω–∏–µ —Å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
            print("–ü–æ–ø—ã—Ç–∫–∞ –æ–±—É—á–µ–Ω–∏—è —Å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏...")
            callbacks_simple = [
                EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),
                ModelCheckpoint('app/ml_models/models/model_checkpoints/best_model_fallback.keras',
                                monitor='val_loss', save_best_only=True)
            ]

            history = model.fit(
                X_train, y_train,
                batch_size=self.config.model_config['batch_size'],
                epochs=min(epochs, 50),  # –£–º–µ–Ω—å—à–∞–µ–º —ç–ø–æ—Ö–∏ –¥–ª—è fallback
                validation_data=(X_val, y_val),
                callbacks=callbacks_simple,
                verbose=1,
                shuffle=False
            )
            return history

    def save_model(self, model, path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Å–∫–µ–π–ª–µ—Ä–æ–≤"""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        model.save(path)
        scaler_path = path.replace('.keras', '_scalers.pkl')
        joblib.dump(self.scalers, scaler_path)
        print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {path}")
        print(f"–°–∫–µ–π–ª–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {scaler_path}")

    def load_model(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Å–∫–µ–π–ª–µ—Ä–æ–≤"""
        model = keras.models.load_model(path)
        scaler_path = path.replace('.keras', '_scalers.pkl')
        if os.path.exists(scaler_path):
            self.scalers = joblib.load(scaler_path)
        return

    def calculate_trend_class_weights(self, trend_labels):
        """–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞"""
        import numpy as np

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º one-hot –≤ —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç–∫–∏
        class_labels = np.argmax(trend_labels, axis=1)

        # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        class_counts = np.bincount(class_labels)

        # –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ (–æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É)
        total_samples = len(class_labels)
        num_classes = len(class_counts)

        class_weights = {}
        for i in range(num_classes):
            if class_counts[i] > 0:
                class_weights[i] = total_samples / (num_classes * class_counts[i])
            else:
                class_weights[i] = 1.0  # –ï—Å–ª–∏ –∫–ª–∞—Å—Å–∞ –Ω–µ—Ç, –≤–µ—Å = 1

        print(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ —Ç—Ä–µ–Ω–¥–∞: {class_weights}")
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {dict(zip(range(num_classes), class_counts))}")

        return class_weights